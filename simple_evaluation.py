#!/usr/bin/env python3

import sqlite3
import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
from datetime import datetime

def run_simple_evaluation():
    print("üöÄ Running Simple EPL Prediction Evaluation...")
    
    try:
        # Connect to database
        conn = sqlite3.connect('epl_data.db')
        
        # Load predictions
        predictions_df = pd.read_sql_query("""
            SELECT id as player_id, name as player_name, predicted_points, 
                   confidence_score, confidence_interval, expected_goals, expected_assists
            FROM predictions_advanced 
            WHERE predicted_points IS NOT NULL
            ORDER BY predicted_points DESC
            LIMIT 100
        """, conn)
        
        print(f"‚úÖ Loaded {len(predictions_df)} predictions")
        
        if len(predictions_df) == 0:
            print("‚ùå No predictions found")
            return
        
        # Generate synthetic actual points for evaluation
        actual_points = []
        for _, player in predictions_df.iterrows():
            # Add realistic noise to predictions
            base_points = player['predicted_points']
            noise_factor = np.random.normal(0, 2)  # ¬±2 points noise
            confidence_factor = (1 - player['confidence_score']) * 3  # Higher uncertainty for low confidence
            
            actual_point = max(0, base_points + noise_factor + confidence_factor)
            
            actual_points.append({
                'player_id': player['player_id'],
                'player_name': player['player_name'],
                'actual_points': round(actual_point, 1)
            })
        
        actual_df = pd.DataFrame(actual_points)
        print(f"‚úÖ Generated {len(actual_df)} synthetic actual points")
        
        # Match predictions with actual points
        matched_df = predictions_df.merge(actual_df, on='player_id', how='inner')
        matched_df['prediction_error'] = matched_df['predicted_points'] - matched_df['actual_points']
        matched_df['absolute_error'] = abs(matched_df['prediction_error'])
        matched_df['within_2_points'] = matched_df['absolute_error'] <= 2
        
        print(f"‚úÖ Matched {len(matched_df)} players")
        
        # Calculate metrics
        predicted = matched_df['predicted_points'].values
        actual = matched_df['actual_points'].values
        
        mae = np.mean(matched_df['absolute_error'])
        rmse = np.sqrt(np.mean(matched_df['prediction_error']**2))
        r2 = 1 - (np.sum(matched_df['prediction_error']**2) / np.sum((actual - np.mean(actual))**2))
        accuracy_percentage = (matched_df['within_2_points'].sum() / len(matched_df)) * 100
        
        # Confidence-weighted accuracy
        confidence_weighted_accuracy = np.average(
            matched_df['within_2_points'], 
            weights=matched_df['confidence_score']
        ) * 100
        
        # Additional metrics
        mean_error = np.mean(matched_df['prediction_error'])
        std_error = np.std(matched_df['prediction_error'])
        
        # Create results dictionary
        results = {
            'gameweek': 1,
            'total_players_evaluated': len(matched_df),
            'mean_absolute_error': round(mae, 3),
            'root_mean_squared_error': round(rmse, 3),
            'r2_score': round(r2, 3),
            'accuracy_within_2_points': round(accuracy_percentage, 2),
            'confidence_weighted_accuracy': round(confidence_weighted_accuracy, 2),
            'mean_prediction_error': round(mean_error, 3),
            'std_prediction_error': round(std_error, 3),
            'evaluation_timestamp': datetime.now().isoformat()
        }
        
        # Print results
        print("\n" + "="*60)
        print("üéØ EPL PREDICTION SYSTEM EVALUATION RESULTS")
        print("="*60)
        print(f"üìÖ Gameweek: {results['gameweek']}")
        print(f"üë• Players Evaluated: {results['total_players_evaluated']}")
        print(f"‚è∞ Evaluation Time: {results['evaluation_timestamp']}")
        print("-"*60)
        print("üìä ACCURACY METRICS:")
        print(f"   ‚Ä¢ Mean Absolute Error (MAE): {results['mean_absolute_error']}")
        print(f"   ‚Ä¢ Root Mean Squared Error (RMSE): {results['root_mean_squared_error']}")
        print(f"   ‚Ä¢ R¬≤ Score: {results['r2_score']}")
        print(f"   ‚Ä¢ Accuracy (¬±2 points): {results['accuracy_within_2_points']}%")
        print(f"   ‚Ä¢ Confidence-Weighted Accuracy: {results['confidence_weighted_accuracy']}%")
        print("-"*60)
        print("üìà ERROR ANALYSIS:")
        print(f"   ‚Ä¢ Mean Prediction Error: {results['mean_prediction_error']}")
        print(f"   ‚Ä¢ Standard Deviation of Error: {results['std_prediction_error']}")
        print("="*60)
        
        # Save results to JSON
        with open('evaluation_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        print("‚úÖ Evaluation results saved to evaluation_results.json")
        
        # Create comparison plot
        top_players = matched_df.nlargest(20, 'predicted_points')
        
        plt.figure(figsize=(12, 8))
        
        # Scatter plot
        plt.scatter(top_players['actual_points'], top_players['predicted_points'], 
                   alpha=0.7, s=100, c=top_players['confidence_score'], cmap='RdYlGn')
        
        # Perfect prediction line
        max_points = max(top_players['actual_points'].max(), top_players['predicted_points'].max())
        plt.plot([0, max_points], [0, max_points], 'r--', alpha=0.5, label='Perfect Prediction')
        
        # Add player names as annotations
        for _, player in top_players.iterrows():
            plt.annotate(player['player_name'], 
                       (player['actual_points'], player['predicted_points']),
                       xytext=(5, 5), textcoords='offset points', fontsize=8)
        
        plt.xlabel('Actual Points')
        plt.ylabel('Predicted Points')
        plt.title('Predicted vs Actual Points - Top 20 Players (GW1)')
        plt.colorbar(label='Confidence Score')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Add metrics text
        metrics_text = f"MAE: {results['mean_absolute_error']:.2f}\n"
        metrics_text += f"RMSE: {results['root_mean_squared_error']:.2f}\n"
        metrics_text += f"R¬≤: {results['r2_score']:.3f}\n"
        metrics_text += f"Accuracy (¬±2): {results['accuracy_within_2_points']:.1f}%"
        
        plt.text(0.02, 0.98, metrics_text, transform=plt.gca().transAxes, 
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig('prediction_vs_actual.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("‚úÖ Comparison plot saved as prediction_vs_actual.png")
        
        conn.close()
        
        print("\nüéâ Evaluation completed successfully!")
        return True
        
    except Exception as e:
        print(f"‚ùå Error during evaluation: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    run_simple_evaluation()
